{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109021e2",
   "metadata": {},
   "source": [
    "# PyTorch binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00563fff",
   "metadata": {},
   "source": [
    "# train_binary.py — train & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ee431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\19452\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\19452\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\19452\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\19452\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19452\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\19452\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Correct for Jupyter\n",
    "!pip install torch pandas scikit-learn joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a6fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN rows: 151565, features: 64596\n",
      "First TRAIN columns: ['id', 'member_id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'desc', 'purpose', 'percent_bc_gt_75', 'bc_util']\n",
      "epoch 001 | train_loss=1.1932 | val_loss=1.1720 | val_auc=0.7283\n"
     ]
    }
   ],
   "source": [
    "# ================== PARAMETERS (EDIT THESE) ==================\n",
    "TRAIN_CSV = r\"C:\\Users\\19452\\Downloads\\Take Home Project\\Take Home Project\\training_loan_data.csv\"\n",
    "TEST_CSV  = r\"C:\\Users\\19452\\Downloads\\Take Home Project\\Take Home Project\\testing_loan_data.csv\"\n",
    "TARGET    = \"bad_flag\"        # resolved case-insensitively\n",
    "SKIPROWS  = 1                 # set to 1 if the first CSV line is not the header\n",
    "ARTIFACTS = \"artifacts\"\n",
    "\n",
    "HIDDEN_DIM   = 128\n",
    "DROPOUT      = 0.10\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 256\n",
    "LR           = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "THRESHOLD    = 0.5\n",
    "# =============================================================\n",
    "\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "os.makedirs(ARTIFACTS, exist_ok=True)\n",
    "\n",
    "# -------------------- Header / Schema + Numeric-ish Coercion --------------------\n",
    "import re\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (df.columns.astype(str)\n",
    "                  .str.replace(r'[\\r\\n\\t]+', ' ', regex=True)\n",
    "                  .str.replace(u'\\xa0', ' ', regex=False)\n",
    "                  .str.replace(' +', ' ', regex=True)\n",
    "                  .str.strip())\n",
    "    return df\n",
    "\n",
    "def resolve_target_column(df: pd.DataFrame, target_name: str) -> str:\n",
    "    if target_name in df.columns:\n",
    "        return target_name\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    if target_name.lower() in low:\n",
    "        return low[target_name.lower()]\n",
    "    raise KeyError(f\"Target '{target_name}' not found. First columns: {df.columns.tolist()[:12]}\")\n",
    "\n",
    "def add_missing_columns(df: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "def parse_numericish_series(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype != object:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    t = s.astype(str).str.strip()\n",
    "    t = (t.str.replace(r'years?', '', regex=True)\n",
    "           .str.replace(r'yrs?', '', regex=True)\n",
    "           .str.replace(r'year', '', regex=True)\n",
    "           .str.replace(r'months?', '', regex=True))\n",
    "    t = (t.str.replace(r'[%$,]', '', regex=True)\n",
    "           .str.replace(r',', '', regex=True))\n",
    "    t = (t.str.replace(r'<\\s*1', '0.5', regex=True)\n",
    "           .str.replace(r'\\+$', '', regex=True))\n",
    "    num = t.str.extract(r'([-+]?\\d*\\.?\\d+)', expand=False)\n",
    "    return pd.to_numeric(num, errors=\"coerce\")\n",
    "\n",
    "def coerce_numericish_cols(df: pd.DataFrame, min_fraction: float = 0.8) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            parsed = parse_numericish_series(df[c])\n",
    "            if parsed.notna().mean() >= min_fraction:\n",
    "                df[c] = parsed.astype(float)\n",
    "    return df\n",
    "\n",
    "# -------------------- sklearn preprocessing (kept sparse) --------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame):\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    for c in list(num_cols):\n",
    "        if pd.api.types.is_integer_dtype(X[c]) and X[c].nunique(dropna=True) <= 10:\n",
    "            num_cols.remove(c); cat_cols.append(c)\n",
    "\n",
    "    numeric = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),   # sparse-friendly\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        ohe = OneHotEncoder(\n",
    "            handle_unknown=\"infrequent_if_exist\",\n",
    "            min_frequency=0.01,\n",
    "            sparse_output=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "    categorical = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric, num_cols),\n",
    "        (\"cat\", categorical, cat_cols),\n",
    "    ])\n",
    "\n",
    "    return preprocessor, num_cols, cat_cols\n",
    "\n",
    "def enforce_schema(df: pd.DataFrame, num_cols: list, cat_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"Force train-defined numeric columns to numeric (strings->NaN), categorical to object.\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"object\")\n",
    "    return df\n",
    "\n",
    "# -------------------- PyTorch model & loaders --------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)  # logits\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class IndexDataset(Dataset):\n",
    "    def __init__(self, n, y=None):\n",
    "        self.n = n\n",
    "        self.y = None if y is None else y.astype(np.float32).reshape(-1, 1)\n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None: return idx\n",
    "        return idx, self.y[idx]\n",
    "\n",
    "class CSRCollator:\n",
    "    def __init__(self, X_csr, y=None):\n",
    "        self.X = X_csr\n",
    "        self.y = y\n",
    "    def __call__(self, batch):\n",
    "        if self.y is None:\n",
    "            idxs = batch\n",
    "            Xa = self.X[idxs].toarray().astype(np.float32)\n",
    "            return torch.from_numpy(Xa)\n",
    "        else:\n",
    "            idxs, ys = zip(*batch)\n",
    "            Xa = self.X[list(idxs)].toarray().astype(np.float32)\n",
    "            Ya = np.vstack(ys).astype(np.float32)\n",
    "            return torch.from_numpy(Xa), torch.from_numpy(Ya)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses, ys, ps = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            prob = torch.sigmoid(logits)\n",
    "            losses.append(loss.item())\n",
    "            ys.append(yb.cpu().numpy())\n",
    "            ps.append(prob.cpu().numpy())\n",
    "    y_true = np.vstack(ys).ravel()\n",
    "    y_prob = np.vstack(ps).ravel()\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return float(np.mean(losses)), auc, y_true, y_prob\n",
    "\n",
    "# -------------------- LOAD TRAIN (normalize + coerce numeric-ish) --------------------\n",
    "df_train = pd.read_csv(TRAIN_CSV, engine=\"python\", skiprows=SKIPROWS)\n",
    "df_train = normalize_columns(df_train)\n",
    "TARGET = resolve_target_column(df_train, TARGET)\n",
    "\n",
    "df_train = coerce_numericish_cols(df_train)\n",
    "\n",
    "y = pd.to_numeric(df_train[TARGET], errors=\"coerce\")\n",
    "mask = y.isin([0, 1])\n",
    "df_train = df_train.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].astype(int).values\n",
    "X = df_train.drop(columns=[TARGET])\n",
    "\n",
    "# keep TRAIN feature list for test alignment\n",
    "train_feature_cols = X.columns.tolist()\n",
    "\n",
    "# -------------------- Preprocessor & Schema Enforcement --------------------\n",
    "preprocessor, num_cols, cat_cols = build_preprocessor(X)\n",
    "\n",
    "# enforce schema on TRAIN features before split/fit\n",
    "X = enforce_schema(X, num_cols, cat_cols)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# enforce schema on splits\n",
    "X_train = enforce_schema(X_train, num_cols, cat_cols)\n",
    "X_val   = enforce_schema(X_val,   num_cols, cat_cols)\n",
    "\n",
    "X_train_csr = preprocessor.fit_transform(X_train)    # CSR matrix\n",
    "X_val_csr   = preprocessor.transform(X_val)          # CSR matrix\n",
    "input_dim   = X_train_csr.shape[1]\n",
    "print(f\"TRAIN rows: {X_train_csr.shape[0]}, features: {input_dim}\")\n",
    "print(\"First TRAIN columns:\", X.columns.tolist()[:12])\n",
    "\n",
    "# -------------------- DataLoaders --------------------\n",
    "train_ds = IndexDataset(len(y_train), y_train)\n",
    "val_ds   = IndexDataset(len(y_val),   y_val)\n",
    "\n",
    "train_collate = CSRCollator(X_train_csr, y_train)\n",
    "val_collate   = CSRCollator(X_val_csr,   y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=train_collate,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=min(1024, BATCH_SIZE * 2),\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=val_collate,\n",
    ")\n",
    "\n",
    "# -------------------- Train --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleMLP(input_dim=input_dim, hidden_dim=HIDDEN_DIM, dropout=DROPOUT).to(device)\n",
    "\n",
    "pos = int(y_train.sum()); neg = len(y_train) - pos\n",
    "pos_weight = torch.tensor([(neg / max(pos, 1))], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_auc, best_state, y_true, y_prob = -1.0, None, None, None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    ep_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ep_losses.append(loss.item())\n",
    "\n",
    "    val_loss, val_auc, y_true, y_prob = evaluate(model, val_loader, criterion, device)\n",
    "    if not math.isnan(val_auc) and val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"epoch {epoch:03d} | train_loss={np.mean(ep_losses):.4f} | val_loss={val_loss:.4f} | val_auc={val_auc:.4f}\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    _, _, y_true, y_prob = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "# Metrics @ threshold\n",
    "y_pred = (y_prob >= THRESHOLD).astype(int)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nBest Val AUC:\", f\"{best_auc:.4f}\")\n",
    "print(\"Acc/Prec/Rec/F1:\", f\"{acc:.4f}\", f\"{prec:.4f}\", f\"{rec:.4f}\", f\"{f1:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Save artifacts\n",
    "import joblib\n",
    "torch.save(model.state_dict(), os.path.join(ARTIFACTS, \"simple_mlp.pt\"))\n",
    "joblib.dump(preprocessor, os.path.join(ARTIFACTS, \"preprocessor.joblib\"))\n",
    "with open(os.path.join(ARTIFACTS, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"val_auc\": float(best_auc), \"acc\": float(acc), \"prec\": float(prec),\n",
    "               \"rec\": float(rec), \"f1\": float(f1)}, f, indent=2)\n",
    "print(f\"Saved model & preprocessor to: {ARTIFACTS}\")\n",
    "\n",
    "# -------------------- LOAD TEST (normalize + coerce + align + enforce schema) --------------------\n",
    "df_test = pd.read_csv(TEST_CSV, engine=\"python\", skiprows=SKIPROWS)\n",
    "df_test = normalize_columns(df_test)\n",
    "df_test = coerce_numericish_cols(df_test)\n",
    "\n",
    "df_test_aligned = add_missing_columns(df_test, train_feature_cols)\n",
    "df_test_aligned = enforce_schema(df_test_aligned, num_cols, cat_cols)  # <-- key fix\n",
    "\n",
    "X_test_csr = preprocessor.transform(df_test_aligned)\n",
    "\n",
    "class OnlyIndexDataset(Dataset):\n",
    "    def __init__(self, n): self.n = n\n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, i): return i\n",
    "\n",
    "test_ds = OnlyIndexDataset(X_test_csr.shape[0])\n",
    "test_collate = CSRCollator(X_test_csr, None)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=min(2048, BATCH_SIZE * 4),\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=test_collate,\n",
    ")\n",
    "\n",
    "probs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        p = torch.sigmoid(model(xb)).cpu().numpy().ravel()\n",
    "        probs.append(p)\n",
    "probs = np.concatenate(probs)\n",
    "preds = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "id_col = \"id\" if \"id\" in df_test_aligned.columns else df_test_aligned.columns[0]\n",
    "pred_df = pd.DataFrame({id_col: df_test_aligned[id_col], \"bad_flag_pred_prob\": probs, \"bad_flag_pred\": preds})\n",
    "pred_path = os.path.join(ARTIFACTS, \"predictions.csv\")\n",
    "pred_df.to_csv(pred_path, index=False)\n",
    "print(f\"Saved predictions to: {pred_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac49d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
